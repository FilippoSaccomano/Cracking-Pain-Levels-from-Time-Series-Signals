{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udff4\u200d\u2620\ufe0f Ensemble di 9 Modelli Migliori - PyTorch\\n",
        "\\n",
        "Questo notebook implementa un ensemble di **9 modelli** (3 configurazioni \u00d7 3 seeds) usando:\\n",
        "- Architettura **Conv1D + BiLSTM**\\n",
        "- Pesatura **EWA (Exponentially Weighted Average)** basata su F1 validation\\n",
        "- Predizione con **soft voting pesato**\\n",
        "\\n",
        "## Pipeline Completa\\n",
        "1. Caricamento dati\\n",
        "2. Preprocessing (ADVICE 07/11, 11/11, 12/11)\\n",
        "3. Creazione finestre\\n",
        "4. **Definizione architettura Conv1D + BiLSTM**\\n",
        "5. **Training ensemble 9 modelli**\\n",
        "6. **Valutazione e calcolo pesi EWA**\\n",
        "7. **Predizione test con ensemble**\\n",
        "8. Generazione submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.9.0+cpu\n",
            "Device: cpu\n",
            "\u2705 Environment ready!\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Stats and ML\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Set seeds\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'Device: {device}')\n",
        "print('\u2705 Environment ready!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "load_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcca Dataset Shape:\n",
            "  Features: (105760, 40)\n",
            "  Labels: (661, 2)\n",
            "  Samples: 661\n",
            "  Timesteps/sample: 160\n",
            "\n",
            "\ud83d\udccb Features: 4 pain_survey + 3 categorical + 31 joints\n",
            "\n",
            "\ud83c\udff7\ufe0f Labels (IMBALANCED - need class weighting):\n",
            "  no_pain: 511 (77.3%)\n",
            "  low_pain: 94 (14.2%)\n",
            "  high_pain: 56 (8.5%)\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "X_train = pd.read_csv('pirate_pain_train.csv')\n",
        "y_train = pd.read_csv('pirate_pain_train_labels.csv')\n",
        "\n",
        "print('\ud83d\udcca Dataset Shape:')\n",
        "print(f'  Features: {X_train.shape}')\n",
        "print(f'  Labels: {y_train.shape}')\n",
        "print(f'  Samples: {X_train[\"sample_index\"].nunique()}')\n",
        "print(f'  Timesteps/sample: {X_train.groupby(\"sample_index\").size().iloc[0]}')\n",
        "\n",
        "# Feature groups\n",
        "pain_survey_cols = [c for c in X_train.columns if 'pain_survey' in c]\n",
        "categorical_cols = ['n_legs', 'n_hands', 'n_eyes']\n",
        "joint_cols = [c for c in X_train.columns if 'joint_' in c]\n",
        "\n",
        "print(f'\\n\ud83d\udccb Features: {len(pain_survey_cols)} pain_survey + {len(categorical_cols)} categorical + {len(joint_cols)} joints')\n",
        "\n",
        "# ADVICE 08/11: Check class imbalance\n",
        "print(f'\\n\ud83c\udff7\ufe0f Labels (IMBALANCED - need class weighting):')\n",
        "for label, count in y_train['label'].value_counts().items():\n",
        "    print(f'  {label}: {count} ({100*count/len(y_train):.1f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "autocorr_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd0d Analyzing autocorrelation...\n",
            "\u2705 WINDOW_SIZE from autocorrelation: 40\n",
            "   STRIDE: 20\n",
            "\ud83d\udca1 ADVICE 11/11: Data-driven window size!\n"
          ]
        }
      ],
      "source": [
        "# ADVICE 11/11: Analyze autocorrelation to determine optimal window\n",
        "print('\ud83d\udd0d Analyzing autocorrelation...')\n",
        "samples_analyze = X_train['sample_index'].unique()[:10]\n",
        "key_features = joint_cols[:6]\n",
        "\n",
        "optimal_lags = {}\n",
        "for feature in key_features:\n",
        "    sample_lags = []\n",
        "    for sid in samples_analyze:\n",
        "        data = X_train[X_train['sample_index']==sid][feature].values\n",
        "        if len(data) >= 50:\n",
        "            max_lags = min(len(data)//2-1, 80)\n",
        "            acf_vals = acf(data, nlags=max_lags)\n",
        "            sig_bound = 1.96/np.sqrt(len(data))\n",
        "            for lag in range(1, len(acf_vals)):\n",
        "                if abs(acf_vals[lag]) < sig_bound:\n",
        "                    sample_lags.append(lag)\n",
        "                    break\n",
        "            else:\n",
        "                sample_lags.append(max_lags)\n",
        "    if sample_lags:\n",
        "        optimal_lags[feature] = int(np.median(sample_lags))\n",
        "\n",
        "if optimal_lags:\n",
        "    suggested = int(np.median(list(optimal_lags.values())))\n",
        "    WINDOW_SIZE = max(min(suggested, 100), 40)\n",
        "else:\n",
        "    WINDOW_SIZE = 60\n",
        "\n",
        "WINDOW_STRIDE = WINDOW_SIZE // 2\n",
        "\n",
        "print(f'\u2705 WINDOW_SIZE from autocorrelation: {WINDOW_SIZE}')\n",
        "print(f'   STRIDE: {WINDOW_STRIDE}')\n",
        "print(f'\ud83d\udca1 ADVICE 11/11: Data-driven window size!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "preproc_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Preprocessing done:\n",
            "   - ADVICE 07/11: Categorical mapped\n",
            "   - ADVICE 12/11: Time features (sin, cos, norm) added\n",
            "   Shape: (105760, 43)\n"
          ]
        }
      ],
      "source": [
        "# ADVICE 07/11: Map categorical features\n",
        "cat_map = {\n",
        "    'n_legs': {'two': 0, 'one+peg_leg': 1},\n",
        "    'n_hands': {'two': 0, 'one+hook_hand': 1},\n",
        "    'n_eyes': {'two': 0, 'one+eye_patch': 1}\n",
        "}\n",
        "\n",
        "X_proc = X_train.copy()\n",
        "for col, mapping in cat_map.items():\n",
        "    X_proc[col] = X_proc[col].map(mapping).fillna(0).astype(int)\n",
        "\n",
        "# ADVICE 12/11: Add cyclical time features\n",
        "max_time = X_proc['time'].max()\n",
        "X_proc['time_sin'] = np.sin(2*np.pi*X_proc['time']/max_time)\n",
        "X_proc['time_cos'] = np.cos(2*np.pi*X_proc['time']/max_time)\n",
        "X_proc['time_norm'] = X_proc['time']/max_time\n",
        "\n",
        "print('\u2705 Preprocessing done:')\n",
        "print('   - ADVICE 07/11: Categorical mapped')\n",
        "print('   - ADVICE 12/11: Time features (sin, cos, norm) added')\n",
        "print(f'   Shape: {X_proc.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "windows_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd04 Creating windows...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Windows: (4627, 40, 41)\n",
            "   Labels: (4627,)\n"
          ]
        }
      ],
      "source": [
        "# Create sliding windows\n",
        "def create_windows(df, sample_idx, window_size, stride):\n",
        "    sample = df[df['sample_index']==sample_idx].sort_values('time')\n",
        "    feat_cols = [c for c in sample.columns if c not in ['sample_index','time']]\n",
        "    features = sample[feat_cols].values\n",
        "    \n",
        "    windows = []\n",
        "    for start in range(0, max(1, len(features)-window_size+1), stride):\n",
        "        end = min(start+window_size, len(features))\n",
        "        win = features[start:end]\n",
        "        if len(win) < window_size:\n",
        "            pad = np.zeros((window_size-len(win), win.shape[1]))\n",
        "            win = np.vstack([win, pad])\n",
        "        windows.append(win)\n",
        "    return windows\n",
        "\n",
        "print('\ud83d\udd04 Creating windows...')\n",
        "all_windows = []\n",
        "all_labels = []\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_train['label'])\n",
        "\n",
        "for sid, label in zip(y_train['sample_index'], y_encoded):\n",
        "    wins = create_windows(X_proc, sid, WINDOW_SIZE, WINDOW_STRIDE)\n",
        "    all_windows.extend(wins)\n",
        "    all_labels.extend([label]*len(wins))\n",
        "\n",
        "X_windows = np.array(all_windows, dtype=np.float32)\n",
        "y_windows = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "print(f'\u2705 Windows: {X_windows.shape}')\n",
        "print(f'   Labels: {y_windows.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "split_code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcca Split: Train (3701, 40, 41), Val (926, 40, 41)\n",
            "\n",
            "\u2696\ufe0f ADVICE 08/11 - Class Weights:\n",
            "   high_pain: 3.929\n",
            "   low_pain: 2.345\n",
            "   no_pain: 0.431\n",
            "\n",
            "\u2705 DataLoaders ready (batch_size=32)\n"
          ]
        }
      ],
      "source": [
        "# Split and normalize\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_windows, y_windows, test_size=0.2, random_state=SEED, stratify=y_windows\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X_tr.reshape(-1, X_tr.shape[-1])).reshape(X_tr.shape)\n",
        "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
        "\n",
        "print(f'\ud83d\udcca Split: Train {X_tr.shape}, Val {X_val.shape}')\n",
        "\n",
        "# ADVICE 08/11: Compute class weights\n",
        "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
        "class_weights_tensor = torch.FloatTensor(class_weights_array).to(device)\n",
        "\n",
        "print(f'\\n\u2696\ufe0f ADVICE 08/11 - Class Weights:')\n",
        "for i, w in enumerate(class_weights_array):\n",
        "    print(f'   {label_encoder.classes_[i]}: {w:.3f}')\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(\n",
        "    torch.FloatTensor(X_tr),\n",
        "    torch.LongTensor(y_tr)\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.FloatTensor(X_val),\n",
        "    torch.LongTensor(y_val)\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f'\\n\u2705 DataLoaders ready (batch_size={BATCH_SIZE})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83c\udfaf ENSEMBLE: Definizioni e Utility\\n",
        "\\n",
        "Ora definiamo tutte le funzioni per l'ensemble approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "PyTorch reimplementation of Conv1D + BiLSTM ensemble with EWA weighting.\n",
        "\n",
        "This script trains 9 models (3 configurations \u00d7 3 seeds) and creates\n",
        "an ensemble using Exponentially Weighted Average (EWA) based on validation F1 scores.\n",
        "\n",
        "Assumes that the following variables already exist in the namespace when imported:\n",
        "    X_tr        : (N_train, WINDOW_SIZE, n_features), float32\n",
        "    y_tr_cat    : (N_train, n_classes), one-hot\n",
        "    X_val       : (N_val,   WINDOW_SIZE, n_features), float32\n",
        "    y_val_cat   : (N_val,   n_classes), one-hot\n",
        "    X_test_proc : feature matrix for test (used in create_windows)\n",
        "    X_test      : DataFrame with column \"sample_index\"\n",
        "    WINDOW_SIZE : int\n",
        "    WINDOW_STRIDE: int (for create_windows)\n",
        "    scaler      : fitted sklearn scaler (for test windows)\n",
        "    label_encoder: fitted sklearn LabelEncoder\n",
        "    create_windows: function as in the original code\n",
        "\"\"\"\n",
        "\n",
        "# =========================================================\n",
        "# 1. IMPORTS & GLOBAL CONFIG\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from scipy.stats import mode\n",
        "from scipy.special import softmax\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Device detection: MPS (Mac M4) > CUDA > CPU\n",
        "DEVICE = torch.device(\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else\n",
        "                      \"cuda\" if torch.cuda.is_available() else\n",
        "                      \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2. SEEDING\n",
        "# =========================================================\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. DATASET & DATALOADER HELPERS\n",
        "# =========================================================\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"Dataset for time series data.\n",
        "    \n",
        "    Args:\n",
        "        X: shape (N, T, F) - N samples, T timesteps, F features\n",
        "        y: shape (N,) with class indices (0..C-1), or None for test data\n",
        "    \"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray | None = None):\n",
        "        super().__init__()\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = None if y is None else torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]  # (T, F)\n",
        "        if self.y is None:\n",
        "            return x\n",
        "        return x, self.y[idx]\n",
        "\n",
        "\n",
        "def make_dataloaders(X_tr, y_tr, X_val, y_val, batch_size: int = 16):\n",
        "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
        "    train_ds = TimeSeriesDataset(X_tr, y_tr)\n",
        "    val_ds   = TimeSeriesDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4. MODEL: Conv1D + BiLSTM\n",
        "# =========================================================\n",
        "\n",
        "class ConvLSTMNet(nn.Module):\n",
        "    \"\"\"Conv1D + Bidirectional LSTM network for time series classification.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,       # n_features\n",
        "        conv_dimension: int,  # conv_dim\n",
        "        lstm_units: int,\n",
        "        num_classes: int,\n",
        "        dense_layer: int = 64,\n",
        "        dropout: float = 0.25,\n",
        "        noise_std: float = 0.01,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "        # Conv1D expects (B, C=F, T). We'll permute in forward.\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=conv_dimension,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm1d(conv_dimension)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=conv_dimension, out_channels=conv_dimension,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm1d(conv_dimension)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "        # BiLSTM (return_sequences=False => use last timestep)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=conv_dimension,\n",
        "            hidden_size=lstm_units,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        self.bn_lstm = nn.BatchNorm1d(2 * lstm_units)\n",
        "\n",
        "        self.fc1     = nn.Linear(2 * lstm_units, dense_layer)\n",
        "        self.drop_fc = nn.Dropout(dropout)\n",
        "        self.fc_out  = nn.Linear(dense_layer, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        if self.training and self.noise_std > 0.0:\n",
        "            x = x + torch.randn_like(x) * self.noise_std\n",
        "\n",
        "        # Conv1D blocks\n",
        "        x = x.permute(0, 2, 1)          # (B, F, T)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        # LSTM\n",
        "        x = x.permute(0, 2, 1)          # (B, T', C)\n",
        "        out, _ = self.lstm(x)           # out: (B, T', 2*lstm_units)\n",
        "        x = out[:, -1, :]               # last timestep (return_sequences=False)\n",
        "        x = self.bn_lstm(x)\n",
        "\n",
        "        # Dense head\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop_fc(x)\n",
        "        logits = self.fc_out(x)         # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def build_conv_lstm_model(input_shape, num_classes, conv_dimension, lstm_units, dropout, dense_layer=64):\n",
        "    \"\"\"Build Conv1D + LSTM model.\"\"\"\n",
        "    _, n_features = input_shape  # (WINDOW_SIZE, n_features)\n",
        "    model = ConvLSTMNet(\n",
        "        input_dim=n_features,\n",
        "        conv_dimension=conv_dimension,\n",
        "        lstm_units=lstm_units,\n",
        "        num_classes=num_classes,\n",
        "        dense_layer=dense_layer,\n",
        "        dropout=dropout,\n",
        "        noise_std=0.01,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5. UTILS: PREDICTION HELPERS\n",
        "# =========================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_proba_on_array(model: nn.Module, X: np.ndarray, batch_size: int = 256) -> np.ndarray:\n",
        "    \"\"\"Predict class probabilities on numpy array.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        X: (N, T, F) numpy array\n",
        "        batch_size: batch size for prediction\n",
        "        \n",
        "    Returns:\n",
        "        (N, n_classes) array with softmax probabilities\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ds = TimeSeriesDataset(X, y=None)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    all_probs = []\n",
        "    for xb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        logits = model(xb)           # (B, C)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(all_probs, axis=0)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 6. ENSEMBLE UTILS\n",
        "# =========================================================\n",
        "\n",
        "def majority_voting(models, X: np.ndarray, batch_size: int = 256) -> np.ndarray:\n",
        "    \"\"\"Ensemble prediction using majority voting.\"\"\"\n",
        "    preds = []\n",
        "    for m in models:\n",
        "        probs = predict_proba_on_array(m, X, batch_size=batch_size)\n",
        "        preds.append(np.argmax(probs, axis=1))\n",
        "\n",
        "    preds = np.stack(preds, axis=0)  # (n_models, N)\n",
        "    final = mode(preds, axis=0, keepdims=False).mode\n",
        "    return final\n",
        "\n",
        "\n",
        "def soft_weighted_ensemble_predict(models, X: np.ndarray, weights: np.ndarray, batch_size: int = 256):\n",
        "    \"\"\"Ensemble prediction using weighted soft voting.\n",
        "    \n",
        "    Args:\n",
        "        models: list of PyTorch models\n",
        "        X: input data (N, T, F)\n",
        "        weights: model weights for averaging\n",
        "        batch_size: batch size for prediction\n",
        "        \n",
        "    Returns:\n",
        "        preds: predicted class indices\n",
        "        weighted_probs: weighted probability distribution\n",
        "    \"\"\"\n",
        "    weighted_probs = None\n",
        "    for w, m in zip(weights, models):\n",
        "        probs = predict_proba_on_array(m, X, batch_size=batch_size)  # (N, C)\n",
        "        if weighted_probs is None:\n",
        "            weighted_probs = w * probs\n",
        "        else:\n",
        "            weighted_probs += w * probs\n",
        "\n",
        "    weighted_probs = softmax(weighted_probs, axis=1)\n",
        "    preds = np.argmax(weighted_probs, axis=1)\n",
        "    return preds, weighted_probs\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 7. TRAINING LOOP + EARLY STOPPING\n",
        "# =========================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping based on validation metric.\"\"\"\n",
        "    \n",
        "    def __init__(self, patience: int = 10, mode: str = \"max\"):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.best_state = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, score: float, model: nn.Module) -> bool:\n",
        "        \"\"\"Returns True if training should stop.\"\"\"\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_state = deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return False\n",
        "\n",
        "        improve = score > self.best_score if self.mode == \"max\" else score < self.best_score\n",
        "\n",
        "        if improve:\n",
        "            self.best_score = score\n",
        "            self.best_state = deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "\n",
        "def train_one_model(\n",
        "    cfg: dict,\n",
        "    seed: int,\n",
        "    X_tr: np.ndarray,\n",
        "    y_tr: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    batch_size: int = 16,\n",
        "):\n",
        "    \"\"\"Train a single model with given hyperparameters and seed.\n",
        "    \n",
        "    Args:\n",
        "        cfg: configuration dict with hyperparameters\n",
        "        seed: random seed\n",
        "        X_tr, y_tr: training data\n",
        "        X_val, y_val: validation data\n",
        "        batch_size: batch size for training\n",
        "        \n",
        "    Returns:\n",
        "        model: trained model with best weights\n",
        "        history: training history\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    gc.collect()\n",
        "\n",
        "    n_features = X_tr.shape[2]\n",
        "    n_classes = len(np.unique(y_tr))\n",
        "    window_size = X_tr.shape[1]\n",
        "\n",
        "    model = build_conv_lstm_model(\n",
        "        input_shape=(window_size, n_features),\n",
        "        num_classes=n_classes,\n",
        "        conv_dimension=cfg[\"conv_dim\"],\n",
        "        lstm_units=cfg[\"lstm_units\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        dense_layer=64,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # optimizer + loss with label smoothing\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=float(cfg[\"label_smoothing\"]))\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=float(cfg[\"lr\"]),\n",
        "        weight_decay=float(cfg[\"wd\"]),\n",
        "    )\n",
        "\n",
        "    train_loader, val_loader = make_dataloaders(X_tr, y_tr, X_val, y_val, batch_size=batch_size)\n",
        "\n",
        "    es = EarlyStopping(patience=cfg[\"early_stop_patience\"], mode=\"max\")\n",
        "    max_epochs = max(100, cfg[\"max_epochs\"])\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # ---- VALIDATION (acc + F1 macro) ----\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                yb = yb.to(DEVICE)\n",
        "\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                preds = probs.argmax(dim=1)\n",
        "\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_targets.append(yb.cpu().numpy())\n",
        "\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_targets = np.concatenate(all_targets)\n",
        "\n",
        "        val_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "        val_acc = (all_preds == all_targets).mean()\n",
        "\n",
        "        epoch_log = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": float(np.mean(train_losses)),\n",
        "            \"val_loss\": float(np.mean(val_losses)),\n",
        "            \"val_acc\": float(val_acc),\n",
        "            \"val_f1\": float(val_f1),\n",
        "        }\n",
        "        history.append(epoch_log)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={epoch_log['train_loss']:.4f} \"\n",
        "              f\"val_loss={epoch_log['val_loss']:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f}\")\n",
        "\n",
        "        # early stopping on val_f1\n",
        "        stop = es.step(val_f1, model)\n",
        "        if stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch} (best val_f1={es.best_score:.4f})\")\n",
        "            break\n",
        "\n",
        "    # restore best weights\n",
        "    if es.best_state is not None:\n",
        "        model.load_state_dict(es.best_state)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8. BEST CONFIGS\n",
        "# =========================================================\n",
        "\n",
        "best_configs = [\n",
        "    {   # trial 223\n",
        "        \"name\": \"128x128x128\",\n",
        "        \"conv_dim\": 128,\n",
        "        \"lstm_units\": 128,\n",
        "        \"dropout\": 0.117753,\n",
        "        \"lr\": 0.000506,\n",
        "        \"label_smoothing\": 0.034647,\n",
        "        \"early_stop_patience\": 999,\n",
        "        \"max_epochs\": 23,\n",
        "        \"wd\": 1e-6,\n",
        "    },\n",
        "    {   # trial 23\n",
        "        \"name\": \"128x128x160\",\n",
        "        \"conv_dim\": 128,\n",
        "        \"lstm_units\": 160,\n",
        "        \"dropout\": 0.107198,\n",
        "        \"lr\": 0.000302,\n",
        "        \"label_smoothing\": 0.051445,\n",
        "        \"early_stop_patience\": 999,\n",
        "        \"max_epochs\": 37,\n",
        "        \"wd\": 8.31e-4,\n",
        "    },\n",
        "    {   # trial 122\n",
        "        \"name\": \"160x160x160\",\n",
        "        \"conv_dim\": 160,\n",
        "        \"lstm_units\": 160,\n",
        "        \"dropout\": 0.114202,\n",
        "        \"lr\": 0.000335,\n",
        "        \"label_smoothing\": 0.051782,\n",
        "        \"early_stop_patience\": 999,\n",
        "        \"max_epochs\": 29,\n",
        "        \"wd\": 3e-5,\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 9. MAIN ENSEMBLE TRAINING FUNCTION\n",
        "# =========================================================\n",
        "\n",
        "def train_ensemble(X_tr, y_tr_cat, X_val, y_val_cat, configs=None, seeds_per_config=3):\n",
        "    \"\"\"Train ensemble of models.\n",
        "    \n",
        "    Args:\n",
        "        X_tr: training features (N, T, F)\n",
        "        y_tr_cat: training labels, one-hot encoded\n",
        "        X_val: validation features (N, T, F)\n",
        "        y_val_cat: validation labels, one-hot encoded\n",
        "        configs: list of config dicts (default: best_configs)\n",
        "        seeds_per_config: number of seeds per config\n",
        "        \n",
        "    Returns:\n",
        "        models: list of trained models\n",
        "        model_info: list of dicts with model metadata\n",
        "        histories: list of training histories\n",
        "    \"\"\"\n",
        "    if configs is None:\n",
        "        configs = best_configs\n",
        "    \n",
        "    # Convert one-hot to indices\n",
        "    y_tr = np.argmax(y_tr_cat, axis=1).astype(np.int64)\n",
        "    y_val = np.argmax(y_val_cat, axis=1).astype(np.int64)\n",
        "\n",
        "    n_features = X_tr.shape[2]\n",
        "    n_classes = len(np.unique(y_tr))\n",
        "\n",
        "    print(\"\u2705 Conv1D + LSTM (PyTorch) ensemble - training start\")\n",
        "    print(f\"   Input: ({X_tr.shape[1]}, {n_features})\")\n",
        "    print(f\"   Output: {n_classes} classes\")\n",
        "\n",
        "    models = []\n",
        "    model_info = []\n",
        "    histories = []\n",
        "\n",
        "    TOTAL_MODELS = len(configs) * seeds_per_config\n",
        "    model_idx = 0\n",
        "\n",
        "    for cfg_idx, cfg in enumerate(configs):\n",
        "        for s in range(seeds_per_config):\n",
        "            model_idx += 1\n",
        "            seed = 100 * cfg_idx + s\n",
        "\n",
        "            print(\"\\n==============================\")\n",
        "            print(f\"  TRAINING MODEL {model_idx}/{TOTAL_MODELS}\")\n",
        "            print(f\"  config: {cfg['name']} | seed: {seed}\")\n",
        "            print(\"==============================\")\n",
        "\n",
        "            model, history = train_one_model(\n",
        "                cfg=cfg,\n",
        "                seed=seed,\n",
        "                X_tr=X_tr,\n",
        "                y_tr=y_tr,\n",
        "                X_val=X_val,\n",
        "                y_val=y_val,\n",
        "                batch_size=16,\n",
        "            )\n",
        "\n",
        "            models.append(model)\n",
        "            model_info.append({\n",
        "                \"name\": f\"{cfg['name']}_seed{seed}\",\n",
        "                \"config\": cfg,\n",
        "                \"seed\": seed,\n",
        "            })\n",
        "            histories.append(history)\n",
        "\n",
        "            gc.collect()\n",
        "\n",
        "    return models, model_info, histories\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 10. ENSEMBLE EVALUATION AND WEIGHTING\n",
        "# =========================================================\n",
        "\n",
        "def evaluate_ensemble(models, X_val, y_val_cat, eta=15.0):\n",
        "    \"\"\"Evaluate ensemble and compute EWA weights.\n",
        "    \n",
        "    Args:\n",
        "        models: list of trained models\n",
        "        X_val: validation features\n",
        "        y_val_cat: validation labels (one-hot)\n",
        "        eta: EWA parameter (higher = more weight to best models)\n",
        "        \n",
        "    Returns:\n",
        "        weights: EWA weights for each model\n",
        "        single_f1: F1 scores for individual models\n",
        "        ensemble_f1: F1 score of the ensemble\n",
        "    \"\"\"\n",
        "    y_val_true = np.argmax(y_val_cat, axis=1)\n",
        "\n",
        "    # Compute F1 for each model\n",
        "    single_f1 = []\n",
        "    for i, m in enumerate(models):\n",
        "        probs = predict_proba_on_array(m, X_val, batch_size=256)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        f1 = f1_score(y_val_true, preds, average=\"macro\")\n",
        "        single_f1.append(f1)\n",
        "        print(f\"Model {i} F1: {f1:.4f}\")\n",
        "\n",
        "    single_f1 = np.array(single_f1)\n",
        "    print(\"\\nF1 scores:\", single_f1)\n",
        "\n",
        "    # Compute EWA weights: Loss = 1 - F1\n",
        "    losses = 1.0 - single_f1\n",
        "    print(\"Losses (1 - F1):\", losses)\n",
        "\n",
        "    raw_weights = np.exp(-eta * losses)\n",
        "    weights = raw_weights / raw_weights.sum()\n",
        "\n",
        "    print(f\"\\nEWA weights (eta = {eta:.1f}):\", weights)\n",
        "    print(\"Sum of weights:\", weights.sum())\n",
        "\n",
        "    # Ensemble prediction\n",
        "    y_val_pred_ewa, _ = soft_weighted_ensemble_predict(models, X_val, weights, batch_size=256)\n",
        "    f1_ens_ewa = f1_score(y_val_true, y_val_pred_ewa, average=\"macro\")\n",
        "    print(f\"\\nEnsemble F1 (soft + EWA, eta={eta}):\", f1_ens_ewa)\n",
        "\n",
        "    print(\"\\n=== Classification Report (Validation, Ensemble EWA) ===\\n\")\n",
        "    print(classification_report(y_val_true, y_val_pred_ewa, digits=4))\n",
        "\n",
        "    print(\"\\n=== Confusion Matrix ===\")\n",
        "    print(confusion_matrix(y_val_true, y_val_pred_ewa))\n",
        "\n",
        "    return weights, single_f1, f1_ens_ewa\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 11. TEST PREDICTION\n",
        "# =========================================================\n",
        "\n",
        "def ensemble_predict_test(models, X_test_proc, df_test, weights, \n",
        "                         WINDOW_SIZE, WINDOW_STRIDE, scaler, create_windows,\n",
        "                         batch_size_windows: int = 256):\n",
        "    \"\"\"Make ensemble predictions on test set.\n",
        "    \n",
        "    Args:\n",
        "        models: list of trained models\n",
        "        X_test_proc: preprocessed test features\n",
        "        df_test: test DataFrame with sample_index column\n",
        "        weights: EWA weights\n",
        "        WINDOW_SIZE: window size\n",
        "        WINDOW_STRIDE: window stride\n",
        "        scaler: fitted scaler\n",
        "        create_windows: window creation function\n",
        "        batch_size_windows: batch size for prediction\n",
        "        \n",
        "    Returns:\n",
        "        results: predicted class indices\n",
        "        test_ids: sample indices\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    test_ids = df_test[\"sample_index\"].unique()\n",
        "\n",
        "    for sid in tqdm(test_ids, desc=\"Predict ensemble (soft+EWA)\"):\n",
        "        # Create windows for this sample\n",
        "        windows = create_windows(X_test_proc, sid, WINDOW_SIZE, WINDOW_STRIDE)\n",
        "\n",
        "        if len(windows) == 0:\n",
        "            results.append(0)\n",
        "            continue\n",
        "\n",
        "        X_sample = np.array(windows, dtype=np.float32)  # (n_windows, T, F)\n",
        "\n",
        "        # Scale windows\n",
        "        X_sample = scaler.transform(\n",
        "            X_sample.reshape(-1, X_sample.shape[-1])\n",
        "        ).reshape(X_sample.shape)\n",
        "\n",
        "        # Weighted average of probabilities across all windows and models\n",
        "        weighted_probs = None\n",
        "\n",
        "        for w, m in zip(weights, models):\n",
        "            probs = predict_proba_on_array(m, X_sample, batch_size=batch_size_windows)  # (n_windows, C)\n",
        "            avg = probs.mean(axis=0)  # average over windows\n",
        "\n",
        "            if weighted_probs is None:\n",
        "                weighted_probs = w * avg\n",
        "            else:\n",
        "                weighted_probs += w * avg\n",
        "\n",
        "        weighted_probs = softmax(weighted_probs)\n",
        "        final_class = int(np.argmax(weighted_probs))\n",
        "        results.append(final_class)\n",
        "\n",
        "    return np.array(results, dtype=int), test_ids\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 12. MAIN EXECUTION FUNCTION\n",
        "# =========================================================\n",
        "\n",
        "def run_ensemble(X_tr, y_tr_cat, X_val, y_val_cat, \n",
        "                 X_test_proc=None, X_test=None, \n",
        "                 WINDOW_SIZE=None, WINDOW_STRIDE=None,\n",
        "                 scaler=None, label_encoder=None, create_windows=None,\n",
        "                 save_submission=True, submission_filename=\"submission_ewa_eta15_pytorch.csv\"):\n",
        "    \"\"\"Run complete ensemble training, evaluation, and prediction.\n",
        "    \n",
        "    Args:\n",
        "        X_tr, y_tr_cat: training data\n",
        "        X_val, y_val_cat: validation data\n",
        "        X_test_proc: preprocessed test features (optional)\n",
        "        X_test: test DataFrame (optional)\n",
        "        WINDOW_SIZE, WINDOW_STRIDE: window parameters\n",
        "        scaler: fitted scaler\n",
        "        label_encoder: fitted label encoder\n",
        "        create_windows: window creation function\n",
        "        save_submission: whether to save submission file\n",
        "        submission_filename: output filename\n",
        "        \n",
        "    Returns:\n",
        "        models: list of trained models\n",
        "        weights: EWA weights\n",
        "        submission: submission DataFrame (if test data provided)\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"ENSEMBLE TRAINING AND EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Train ensemble\n",
        "    models, model_info, histories = train_ensemble(X_tr, y_tr_cat, X_val, y_val_cat)\n",
        "    \n",
        "    # Evaluate and compute weights\n",
        "    weights, single_f1, ensemble_f1 = evaluate_ensemble(models, X_val, y_val_cat, eta=15.0)\n",
        "    \n",
        "    # Test prediction (if test data provided)\n",
        "    submission = None\n",
        "    if all(v is not None for v in [X_test_proc, X_test, WINDOW_SIZE, WINDOW_STRIDE, \n",
        "                                     scaler, label_encoder, create_windows]):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"TEST PREDICTION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        test_pred, test_ids = ensemble_predict_test(\n",
        "            models, X_test_proc, X_test, weights,\n",
        "            WINDOW_SIZE, WINDOW_STRIDE, scaler, create_windows\n",
        "        )\n",
        "\n",
        "        submission = pd.DataFrame({\n",
        "            \"sample_index\": test_ids,\n",
        "            \"label\": label_encoder.inverse_transform(test_pred),\n",
        "        })\n",
        "\n",
        "        if save_submission:\n",
        "            submission.to_csv(submission_filename, index=False)\n",
        "            print(f\"\\n\u2705 Submission saved as '{submission_filename}'\")\n",
        "    else:\n",
        "        print(\"\\n[INFO] Skipped test prediction: missing required variables\")\n",
        "    \n",
        "    return models, weights, submission\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 13. STANDALONE EXECUTION (if run as script)\n",
        "# =========================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\"\"\n",
        "    This script is designed to be imported and run from a notebook.\n",
        "    It expects the following variables to be defined:\n",
        "        - X_tr, y_tr_cat, X_val, y_val_cat\n",
        "        - X_test_proc, X_test (for test prediction)\n",
        "        - WINDOW_SIZE, WINDOW_STRIDE\n",
        "        - scaler, label_encoder, create_windows\n",
        "    \n",
        "    Example usage in notebook:\n",
        "        import ensemble_pytorch\n",
        "        models, weights, submission = ensemble_pytorch.run_ensemble(\n",
        "            X_tr, y_tr_cat, X_val, y_val_cat,\n",
        "            X_test_proc, X_test, WINDOW_SIZE, WINDOW_STRIDE,\n",
        "            scaler, label_encoder, create_windows\n",
        "        )\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83d\ude80 Training Ensemble\\n",
        "\\n",
        "Ora alleniamo i 9 modelli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Prepara i dati per l'ensemble\\n",
        "from sklearn.preprocessing import label_binarize\\n",
        "\\n",
        "# Convert to one-hot\\n",
        "y_tr_cat = label_binarize(y_tr, classes=range(len(label_encoder.classes_)))\\n",
        "y_val_cat = label_binarize(y_val, classes=range(len(label_encoder.classes_)))\\n",
        "\\n",
        "print(f\"\u2705 Dati preparati:\")\\n",
        "print(f\"   X_tr: {X_tr.shape}\")\\n",
        "print(f\"   y_tr_cat: {y_tr_cat.shape}\")\\n",
        "print(f\"   X_val: {X_val.shape}\")\\n",
        "print(f\"   y_val_cat: {y_val_cat.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Train ensemble\\n",
        "print(\"\\n\" + \"=\"*60)\\n",
        "print(\"INIZIO TRAINING ENSEMBLE\")\\n",
        "print(\"=\"*60)\\n",
        "\\n",
        "models, model_info, histories = train_ensemble(\\n",
        "    X_tr, y_tr_cat, X_val, y_val_cat,\\n",
        "    configs=best_configs,\\n",
        "    seeds_per_config=3\\n",
        ")\\n",
        "\\n",
        "print(f\"\\n\u2705 Training completato! {len(models)} modelli allenati.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83d\udcca Valutazione Ensemble e Calcolo Pesi EWA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Evaluate ensemble\\n",
        "print(\"\\n\" + \"=\"*60)\\n",
        "print(\"VALUTAZIONE ENSEMBLE\")\\n",
        "print(\"=\"*60)\\n",
        "\\n",
        "weights, single_f1, ensemble_f1 = evaluate_ensemble(\\n",
        "    models, X_val, y_val_cat, eta=15.0\\n",
        ")\\n",
        "\\n",
        "print(f\"\\n\u2705 F1 Ensemble: {ensemble_f1:.4f}\")\\n",
        "print(f\"   F1 medio singoli modelli: {single_f1.mean():.4f}\")\\n",
        "print(f\"   Miglioramento: {(ensemble_f1 - single_f1.mean())*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83e\uddea Test Prediction con Ensemble\\n",
        "\\n",
        "Ora facciamo le predizioni sul test set usando l'ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load test data\\n",
        "X_test = pd.read_csv('pirate_pain_test.csv')\\n",
        "print(f'\ud83d\udcca Test Data:')\\n",
        "print(f'   Shape: {X_test.shape}')\\n",
        "print(f'   Samples: {X_test[\"sample_index\"].nunique()}')\\n",
        "\\n",
        "# Apply preprocessing to test\\n",
        "X_test_proc = X_test.copy()\\n",
        "\\n",
        "# ADVICE 07/11: Map categorical\\n",
        "for col, mapping in cat_map.items():\\n",
        "    X_test_proc[col] = X_test_proc[col].map(mapping).fillna(0).astype(int)\\n",
        "\\n",
        "# ADVICE 12/11: Add time features\\n",
        "max_time_test = X_test_proc['time'].max()\\n",
        "X_test_proc['time_sin'] = np.sin(2*np.pi*X_test_proc['time']/max_time_test)\\n",
        "X_test_proc['time_cos'] = np.cos(2*np.pi*X_test_proc['time']/max_time_test)\\n",
        "X_test_proc['time_norm'] = X_test_proc['time']/max_time_test\\n",
        "\\n",
        "print('\u2705 Test preprocessing done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Ensemble prediction on test\\n",
        "print(\"\\n\" + \"=\"*60)\\n",
        "print(\"PREDIZIONE TEST CON ENSEMBLE\")\\n",
        "print(\"=\"*60)\\n",
        "\\n",
        "test_pred, test_ids = ensemble_predict_test(\\n",
        "    models, X_test_proc, X_test, weights,\\n",
        "    WINDOW_SIZE, WINDOW_STRIDE, scaler, create_windows,\\n",
        "    batch_size_windows=256\\n",
        ")\\n",
        "\\n",
        "print(f\"\\n\u2705 Predizioni completate per {len(test_pred)} campioni\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83d\udcbe Creazione Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create submission\\n",
        "submission = pd.DataFrame({\\n",
        "    \"sample_index\": test_ids,\\n",
        "    \"label\": label_encoder.inverse_transform(test_pred),\\n",
        "})\\n",
        "\\n",
        "submission.to_csv(\"submission_ensemble_9models.csv\", index=False)\\n",
        "\\n",
        "print(\"\u2705 Submission salvata come 'submission_ensemble_9models.csv'\")\\n",
        "print(f\"\\n\ud83d\udccb Preview:\")\\n",
        "print(submission.head(10))\\n",
        "print(f\"\\nTotal predictions: {len(submission)}\")\\n",
        "print(f\"\\nLabel distribution:\")\\n",
        "print(submission['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83d\udcc8 Analisi Risultati\\n",
        "\\n",
        "Visualizziamo i risultati dell'ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "\\n",
        "# Plot F1 scores\\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\\n",
        "\\n",
        "# Individual model F1 scores\\n",
        "ax1.bar(range(len(single_f1)), single_f1, alpha=0.7)\\n",
        "ax1.axhline(y=ensemble_f1, color='r', linestyle='--', label=f'Ensemble F1: {ensemble_f1:.4f}')\\n",
        "ax1.axhline(y=single_f1.mean(), color='g', linestyle='--', label=f'Mean F1: {single_f1.mean():.4f}')\\n",
        "ax1.set_xlabel('Model Index')\\n",
        "ax1.set_ylabel('F1 Score (macro)')\\n",
        "ax1.set_title('F1 Scores: Individual Models vs Ensemble')\\n",
        "ax1.legend()\\n",
        "ax1.grid(True, alpha=0.3)\\n",
        "\\n",
        "# Model weights\\n",
        "ax2.bar(range(len(weights)), weights, alpha=0.7, color='orange')\\n",
        "ax2.set_xlabel('Model Index')\\n",
        "ax2.set_ylabel('Weight')\\n",
        "ax2.set_title('EWA Weights (eta=15.0)')\\n",
        "ax2.grid(True, alpha=0.3)\\n",
        "\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "print(\"\\n\ud83d\udcca Statistiche:\")\\n",
        "print(f\"   Best individual F1: {single_f1.max():.4f}\")\\n",
        "print(f\"   Worst individual F1: {single_f1.min():.4f}\")\\n",
        "print(f\"   Mean F1: {single_f1.mean():.4f}\")\\n",
        "print(f\"   Ensemble F1: {ensemble_f1:.4f}\")\\n",
        "print(f\"   Improvement over mean: {(ensemble_f1 - single_f1.mean())*100:.2f}%\")\\n",
        "print(f\"   Improvement over best: {(ensemble_f1 - single_f1.max())*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## \ud83c\udf89 Conclusioni\\n",
        "\\n",
        "L'ensemble di 9 modelli \u00e8 stato completato con successo!\\n",
        "\\n",
        "### Vantaggi dell'Ensemble:\\n",
        "1. **Riduzione della varianza**: mediando pi\u00f9 modelli si riduce l'overfitting\\n",
        "2. **Migliore generalizzazione**: seed diversi esplorano diversi minimi locali\\n",
        "3. **Voting pesato**: i modelli migliori hanno pi\u00f9 influenza (EWA)\\n",
        "4. **Robustezza**: meno sensibile a singoli modelli fallimentari\\n",
        "\\n",
        "### File generato:\\n",
        "- `submission_ensemble_9models.csv`: file di submission con predizioni ensemble"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}