{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pirate Pain Level Classification",
    "",
    "## Overview",
    "This notebook implements a deep learning pipeline to classify pain levels in pirates based on time-series sensor data. The classification task involves predicting three pain levels:",
    "- **no_pain**: No pain detected",
    "- **low_pain**: Low level of pain",
    "- **high_pain**: High level of pain",
    "",
    "## Dataset Description",
    "The dataset consists of multivariate time-series data with the following features:",
    "- **Pain survey responses**: 4 self-reported pain indicators (pain_survey_1 to pain_survey_4)",
    "- **Body characteristics**: Number of legs, hands, and eyes",
    "- **Joint angles**: 31 joint measurements (joint_00 to joint_30) captured over time",
    "",
    "Each sample has 160 time steps with measurements recorded at regular intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup",
    "",
    "Import necessary libraries and configure the environment for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel processing\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"\u2705 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration",
    "",
    "Load the training data and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels\n",
    "train_features = pd.read_csv('pirate_pain_train.csv')\n",
    "train_labels = pd.read_csv('pirate_pain_train_labels.csv')\n",
    "\n",
    "print(\"\ud83d\udcca Training Data Overview\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features shape: {train_features.shape}\")\n",
    "print(f\"Labels shape: {train_labels.shape}\")\n",
    "print(f\"Number of unique samples: {train_features['sample_index'].nunique()}\")\n",
    "print(f\"Time steps per sample: {train_features.groupby('sample_index').size().iloc[0]}\")\n",
    "print(f\"Number of features: {train_features.shape[1] - 2}\")  # Excluding sample_index and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of features\n",
    "print(\"\\n\ud83d\udccb First few rows of training features:\")\n",
    "display(train_features.head(10))\n",
    "\n",
    "# Display sample of labels\n",
    "print(\"\\n\ud83c\udff7\ufe0f First few labels:\")\n",
    "display(train_labels.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Class Distribution Analysis",
    "",
    "Examine the distribution of pain levels in the training set to identify any class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "label_distribution = train_labels['label'].value_counts()\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors_bar = {'no_pain': 'green', 'low_pain': 'orange', 'high_pain': 'red'}\n",
    "bar_colors = [colors_bar.get(label, 'gray') for label in label_distribution.index]\n",
    "axes[0].bar(label_distribution.index, label_distribution.values, color=bar_colors)\n",
    "axes[0].set_xlabel('Pain Level', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Pain Levels', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(label_distribution.values, labels=label_distribution.index, \n",
    "            autopct='%1.1f%%', startangle=90, colors=bar_colors)\n",
    "axes[1].set_title('Pain Level Proportions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Class Statistics:\")\n",
    "print(label_distribution)\n",
    "print(f\"\\nClass imbalance ratio: {label_distribution.min() / label_distribution.max():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Analysis",
    "",
    "Identify and categorize the different types of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features\n",
    "pain_survey_features = [col for col in train_features.columns if 'pain_survey' in col]\n",
    "body_characteristic_features = ['n_legs', 'n_hands', 'n_eyes']\n",
    "joint_angle_features = [col for col in train_features.columns if 'joint_' in col]\n",
    "\n",
    "print(\"\ud83d\udd0d Feature Categories:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n\ud83d\udccb Pain Survey Features ({len(pain_survey_features)}):\")\n",
    "print(f\"   {', '.join(pain_survey_features)}\")\n",
    "print(f\"\\n\ud83e\uddcd Body Characteristics ({len(body_characteristic_features)}):\")\n",
    "print(f\"   {', '.join(body_characteristic_features)}\")\n",
    "print(f\"\\n\ud83e\uddb4 Joint Angle Measurements ({len(joint_angle_features)}):\")\n",
    "print(f\"   Joint features: joint_00 to joint_{len(joint_angle_features)-1:02d}\")\n",
    "print(f\"\\n\u2705 Total features: {len(pain_survey_features) + len(body_characteristic_features) + len(joint_angle_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Time Series Visualization",
    "",
    "Visualize the temporal patterns in the data for a sample pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample to visualize\n",
    "sample_id = 0\n",
    "sample_time_series = train_features[train_features['sample_index'] == sample_id].sort_values('time')\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'Time Series Data for Sample {sample_id}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot pain survey readings\n",
    "for feature in pain_survey_features:\n",
    "    axes[0, 0].plot(sample_time_series['time'], sample_time_series[feature], \n",
    "                    marker='o', label=feature, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].set_title('Pain Survey Readings Over Time')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot body characteristics\n",
    "for feature in body_characteristic_features:\n",
    "    if feature in sample_time_series.columns:\n",
    "        axes[0, 1].plot(sample_time_series['time'], sample_time_series[feature], \n",
    "                        marker='s', label=feature, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].set_title('Body Characteristics Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot first 5 joint angles\n",
    "for feature in joint_angle_features[:5]:\n",
    "    axes[1, 0].plot(sample_time_series['time'], sample_time_series[feature], \n",
    "                    label=feature, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].set_ylabel('Angle (degrees)')\n",
    "axes[1, 0].set_title('Joint Angles (First 5)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot last 5 joint angles\n",
    "for feature in joint_angle_features[-5:]:\n",
    "    axes[1, 1].plot(sample_time_series['time'], sample_time_series[feature], \n",
    "                    label=feature, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].set_ylabel('Angle (degrees)')\n",
    "axes[1, 1].set_title('Joint Angles (Last 5)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing",
    "",
    "### 3.1 Feature Preprocessing",
    "",
    "Define preprocessing functions to handle categorical variables and prepare features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical mapping\n",
    "CATEGORICAL_MAPPING = {'zero': 0, 'one': 1, 'two': 2, 'three': 3}\n",
    "CATEGORICAL_FEATURES = ['n_legs', 'n_hands', 'n_eyes']\n",
    "\n",
    "def preprocess_features(dataframe):\n",
    "    \"\"\"\n",
    "    Preprocess features for model input.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame containing time-series features\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of preprocessed features\n",
    "    \"\"\"\n",
    "    # Exclude identifier columns\n",
    "    excluded_columns = ['sample_index', 'time']\n",
    "    feature_columns = [col for col in dataframe.columns if col not in excluded_columns]\n",
    "    \n",
    "    # Copy relevant features\n",
    "    processed_data = dataframe[feature_columns].copy()\n",
    "    \n",
    "    # Convert categorical features to numeric\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        if feature in processed_data.columns:\n",
    "            # Map categorical values to numeric\n",
    "            mapped_values = processed_data[feature].map(CATEGORICAL_MAPPING)\n",
    "            \n",
    "            # Fill missing values with mode or 0\n",
    "            if mapped_values.isna().all():\n",
    "                fill_value = 0\n",
    "            else:\n",
    "                mode_series = mapped_values.mode(dropna=True)\n",
    "                fill_value = mode_series.iloc[0] if not mode_series.empty else 0\n",
    "            \n",
    "            processed_data[feature] = mapped_values.fillna(fill_value)\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    processed_data = processed_data.fillna(0)\n",
    "    \n",
    "    return processed_data.values\n",
    "\n",
    "print(\"\u2705 Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Sequences",
    "",
    "Convert the data into sequences suitable for LSTM/RNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences from time-series data\n",
    "print(\"\ud83d\udd04 Creating sequences...\")\n",
    "\n",
    "sequences = []\n",
    "sequence_labels = []\n",
    "\n",
    "unique_sample_indices = train_features['sample_index'].unique()\n",
    "\n",
    "for sample_idx in unique_sample_indices:\n",
    "    # Extract all time steps for this sample\n",
    "    sample_data = train_features[train_features['sample_index'] == sample_idx].copy()\n",
    "    sample_data = sample_data.sort_values('time')\n",
    "    \n",
    "    # Preprocess features\n",
    "    features = preprocess_features(sample_data)\n",
    "    sequences.append(features)\n",
    "    \n",
    "    # Get corresponding label\n",
    "    label = train_labels[train_labels['sample_index'] == sample_idx]['label'].iloc[0]\n",
    "    sequence_labels.append(label)\n",
    "\n",
    "print(f\"\u2705 Created {len(sequences)} sequences\")\n",
    "print(f\"   Sequence length range: {min(s.shape[0] for s in sequences)} to {max(s.shape[0] for s in sequences)} time steps\")\n",
    "print(f\"   Features per time step: {sequences[0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Scaling",
    "",
    "Scale features using StandardScaler to normalize the data and improve model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for better model performance\n",
    "print(\"\u2696\ufe0f Scaling features...\")\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "# Fit scaler on all time steps combined\n",
    "feature_scaler.fit(np.vstack(sequences))\n",
    "# Transform each sequence\n",
    "scaled_sequences = [feature_scaler.transform(seq) for seq in sequences]\n",
    "\n",
    "print(\"\u2705 Feature scaling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pad Sequences",
    "",
    "Pad sequences to uniform length for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to uniform length\n",
    "max_sequence_length = max([s.shape[0] for s in scaled_sequences])\n",
    "num_features = scaled_sequences[0].shape[1]\n",
    "\n",
    "print(f\"\ud83d\udccf Padding sequences to maximum length: {max_sequence_length}\")\n",
    "\n",
    "# Create padded array\n",
    "padded_sequences = np.zeros((len(scaled_sequences), max_sequence_length, num_features))\n",
    "for i, sequence in enumerate(scaled_sequences):\n",
    "    padded_sequences[i, :sequence.shape[0], :] = sequence\n",
    "\n",
    "print(f\"\u2705 Padded sequences shape: {padded_sequences.shape}\")\n",
    "print(f\"   Format: (num_samples, time_steps, num_features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Encode Labels",
    "",
    "Convert text labels to numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(sequence_labels)\n",
    "\n",
    "print(\"\ud83c\udff7\ufe0f Label Encoding Mapping:\")\n",
    "for idx, label_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"   {label_name} \u2192 {idx}\")\n",
    "\n",
    "# Show class distribution\n",
    "class_distribution = pd.Series(encoded_labels).value_counts().sort_index()\n",
    "print(\"\\nClass Distribution:\")\n",
    "for class_idx, count in class_distribution.items():\n",
    "    print(f\"   Class {class_idx} ({label_encoder.classes_[class_idx]}): {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Train-Validation Split",
    "",
    "Split data into training and validation sets with stratification to maintain class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified train-validation split\n",
    "print(\"\u2702\ufe0f Splitting data into train and validation sets...\")\n",
    "\n",
    "train_sequences, val_sequences, train_labels_encoded, val_labels_encoded = train_test_split(\n",
    "    padded_sequences, encoded_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=encoded_labels\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Training set: {train_sequences.shape[0]} samples\")\n",
    "print(f\"\u2705 Validation set: {val_sequences.shape[0]} samples\")\n",
    "\n",
    "# Store dimensions for model building\n",
    "sequence_length = train_sequences.shape[1]\n",
    "feature_count = train_sequences.shape[2]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Show distribution in splits\n",
    "train_dist = pd.Series(train_labels_encoded).value_counts().sort_index()\n",
    "val_dist = pd.Series(val_labels_encoded).value_counts().sort_index()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Class Distribution After Split:\")\n",
    "for i, label_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"   {label_name}: Train={train_dist.get(i, 0)}, Val={val_dist.get(i, 0)}\")\n",
    "\n",
    "# Store base training data for resampling experiments\n",
    "base_train_sequences = train_sequences.astype(np.float32)\n",
    "base_train_labels = train_labels_encoded.copy()\n",
    "\n",
    "print(\"\\n\u2705 Base training data stored for balancing strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture and Training Components",
    "",
    "### 4.1 Custom F1-Score Metric",
    "",
    "Implement a custom F1-score metric for Keras to monitor macro-averaged F1-score during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    Custom F1-Score metric for multi-class classification.\n",
    "    Computes macro-averaged F1-score across all classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, name='f1_score', num_classes=3, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize state variables for each class\n",
    "        self.true_positives = self.add_weight(\n",
    "            name='tp', shape=(num_classes,), initializer='zeros'\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name='fp', shape=(num_classes,), initializer='zeros'\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name='fn', shape=(num_classes,), initializer='zeros'\n",
    "        )\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Update metric state with new predictions.\"\"\"\n",
    "        # Convert predictions to class indices\n",
    "        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "        y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "        \n",
    "        # Create one-hot encodings\n",
    "        y_true_one_hot = tf.cast(tf.one_hot(y_true, depth=self.num_classes), tf.float32)\n",
    "        y_pred_one_hot = tf.cast(tf.one_hot(y_pred_classes, depth=self.num_classes), tf.float32)\n",
    "        \n",
    "        # Apply sample weights if provided\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(tf.reshape(sample_weight, [-1, 1]), tf.float32)\n",
    "            y_true_one_hot *= sample_weight\n",
    "            y_pred_one_hot *= sample_weight\n",
    "        \n",
    "        # Calculate TP, FP, FN\n",
    "        tp = tf.reduce_sum(y_true_one_hot * y_pred_one_hot, axis=0)\n",
    "        fp = tf.reduce_sum((1.0 - y_true_one_hot) * y_pred_one_hot, axis=0)\n",
    "        fn = tf.reduce_sum(y_true_one_hot * (1.0 - y_pred_one_hot), axis=0)\n",
    "        \n",
    "        # Update state\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\"Compute macro-averaged F1-score.\"\"\"\n",
    "        precision = self.true_positives / (\n",
    "            self.true_positives + self.false_positives + tf.keras.backend.epsilon()\n",
    "        )\n",
    "        recall = self.true_positives / (\n",
    "            self.true_positives + self.false_negatives + tf.keras.backend.epsilon()\n",
    "        )\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        return tf.reduce_mean(f1)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset metric state.\"\"\"\n",
    "        self.true_positives.assign(tf.zeros((self.num_classes,)))\n",
    "        self.false_positives.assign(tf.zeros((self.num_classes,)))\n",
    "        self.false_negatives.assign(tf.zeros((self.num_classes,)))\n",
    "\n",
    "print(\"\u2705 F1-Score metric class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Builders",
    "",
    "Define functions to build different model architectures: LSTM, GRU, and CNN-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(sequence_length, num_features, num_classes, \n",
    "                     units=(128, 64), dropout=0.3):\n",
    "    \"\"\"\n",
    "    Build a stacked LSTM model for time-series classification.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input sequences\n",
    "        num_features: Number of features per time step\n",
    "        num_classes: Number of output classes\n",
    "        units: Tuple of LSTM layer sizes\n",
    "        dropout: Dropout rate\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Masking(mask_value=0.0, input_shape=(sequence_length, num_features)),\n",
    "        layers.LSTM(units[0], return_sequences=True),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.LSTM(units[1]),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(sequence_length, num_features, num_classes, \n",
    "                    units=(128, 64), dropout=0.3):\n",
    "    \"\"\"\n",
    "    Build a stacked GRU model for time-series classification.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input sequences\n",
    "        num_features: Number of features per time step\n",
    "        num_classes: Number of output classes\n",
    "        units: Tuple of GRU layer sizes\n",
    "        dropout: Dropout rate\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Masking(mask_value=0.0, input_shape=(sequence_length, num_features)),\n",
    "        layers.GRU(units[0], return_sequences=True),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.GRU(units[1]),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_cnn_lstm_model(sequence_length, num_features, num_classes,\n",
    "                         filters=64, kernel_size=3, lstm_units=64, dropout=0.3):\n",
    "    \"\"\"\n",
    "    Build a hybrid CNN-LSTM model for time-series classification.\n",
    "    \n",
    "    CNN layers extract local patterns, LSTM layers capture temporal dependencies.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input sequences\n",
    "        num_features: Number of features per time step\n",
    "        num_classes: Number of output classes\n",
    "        filters: Number of CNN filters\n",
    "        kernel_size: Size of CNN kernel\n",
    "        lstm_units: Number of LSTM units\n",
    "        dropout: Dropout rate\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Masking(mask_value=0.0, input_shape=(sequence_length, num_features)),\n",
    "        layers.Conv1D(filters, kernel_size, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.LSTM(lstm_units),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_model_by_type(model_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory function to build models by type.\n",
    "    \n",
    "    Args:\n",
    "        model_type: One of 'LSTM', 'GRU', or 'CNN_LSTM'\n",
    "        **kwargs: Arguments to pass to the specific builder\n",
    "        \n",
    "    Returns:\n",
    "        Keras model\n",
    "    \"\"\"\n",
    "    if model_type == \"LSTM\":\n",
    "        return build_lstm_model(**kwargs)\n",
    "    elif model_type == \"GRU\":\n",
    "        return build_gru_model(**kwargs)\n",
    "    elif model_type == \"CNN_LSTM\":\n",
    "        return build_cnn_lstm_model(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "print(\"\u2705 Model builder functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Utilities",
    "",
    "Define callbacks and data augmentation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_callbacks():\n",
    "    \"\"\"\n",
    "    Create Keras callbacks for training.\n",
    "    \n",
    "    Returns:\n",
    "        List of callbacks\n",
    "    \"\"\"\n",
    "    return [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_f1_score',\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_f1_score',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            min_lr=1e-6,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def apply_smote_resampling(X_train, y_train, smote_params):\n",
    "    \"\"\"\n",
    "    Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance classes.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training sequences (3D array)\n",
    "        y_train: Training labels\n",
    "        smote_params: SMOTE configuration parameters\n",
    "        \n",
    "    Returns:\n",
    "        Resampled sequences and labels\n",
    "    \"\"\"\n",
    "    smote = SMOTE(**smote_params)\n",
    "    # Flatten sequences for SMOTE\n",
    "    X_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    # Apply SMOTE\n",
    "    X_resampled_flat, y_resampled = smote.fit_resample(X_flat, y_train)\n",
    "    # Reshape back to 3D\n",
    "    X_resampled = X_resampled_flat.reshape(-1, sequence_length, feature_count)\n",
    "    return X_resampled.astype(np.float32), y_resampled\n",
    "\n",
    "print(\"\u2705 Training utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation",
    "",
    "### 5.1 Ensemble Grid Search",
    "",
    "Train multiple model architectures and configurations to find the best performing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude80 Starting ensemble grid search...\")\n",
    "\n",
    "# Define parameter grids\n",
    "smote_parameter_grid = [\n",
    "    {'k_neighbors': 5, 'sampling_strategy': 'auto', 'random_state': 42},\n",
    "]\n",
    "\n",
    "model_parameter_grid = [\n",
    "    {'model_type': 'LSTM', 'units': (128, 64), 'dropout': 0.3, 'learning_rate': 7e-4},\n",
    "    {'model_type': 'GRU', 'units': (128, 64), 'dropout': 0.3, 'learning_rate': 7e-4},\n",
    "    {'model_type': 'CNN_LSTM', 'filters': 64, 'kernel_size': 3, \n",
    "     'lstm_units': 64, 'dropout': 0.3, 'learning_rate': 1e-3},\n",
    "]\n",
    "\n",
    "training_config = {'epochs': 50, 'batch_size': 32}\n",
    "\n",
    "def train_single_model(smote_params, model_params):\n",
    "    \"\"\"\n",
    "    Train a single model configuration.\n",
    "    \n",
    "    Args:\n",
    "        smote_params: SMOTE configuration\n",
    "        model_params: Model hyperparameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing training results\n",
    "    \"\"\"\n",
    "    # Copy parameters to avoid modification\n",
    "    smote_config = smote_params.copy()\n",
    "    model_config = model_params.copy()\n",
    "    \n",
    "    # Extract model-specific parameters\n",
    "    model_type = model_config.pop(\"model_type\")\n",
    "    learning_rate = model_config.pop(\"learning_rate\", 1e-3)\n",
    "    dropout = model_config.pop(\"dropout\", 0.3)\n",
    "    \n",
    "    # Apply SMOTE resampling\n",
    "    X_resampled, y_resampled = apply_smote_resampling(\n",
    "        base_train_sequences, base_train_labels, smote_config\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    build_args = {\n",
    "        'sequence_length': sequence_length,\n",
    "        'num_features': feature_count,\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "    \n",
    "    model = build_model_by_type(\n",
    "        model_type,\n",
    "        **build_args,\n",
    "        dropout=dropout,\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_resampled, y_resampled,\n",
    "        validation_data=(val_sequences, val_labels_encoded),\n",
    "        epochs=training_config['epochs'],\n",
    "        batch_size=training_config['batch_size'],\n",
    "        callbacks=create_training_callbacks(),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_predictions_probs = model.predict(val_sequences, verbose=0)\n",
    "    val_predictions = np.argmax(val_predictions_probs, axis=1)\n",
    "    f1_macro = f1_score(val_labels_encoded, val_predictions, average='macro')\n",
    "    accuracy = np.mean(val_labels_encoded == val_predictions)\n",
    "    \n",
    "    # Extract best epoch info\n",
    "    history_dict = history.history\n",
    "    val_f1_history = history_dict.get('val_f1_score')\n",
    "    best_epoch = int(np.argmax(val_f1_history)) if val_f1_history else None\n",
    "    best_val_f1 = float(max(val_f1_history)) if val_f1_history else None\n",
    "    \n",
    "    # Prepare result\n",
    "    full_model_config = {**model_config}\n",
    "    full_model_config['model_type'] = model_type\n",
    "    full_model_config['dropout'] = dropout\n",
    "    full_model_config['learning_rate'] = learning_rate\n",
    "    \n",
    "    result = {\n",
    "        'model_type': model_type,\n",
    "        'model_config': full_model_config,\n",
    "        'smote_params': smote_config,\n",
    "        'val_f1_macro': f1_macro,\n",
    "        'val_accuracy': accuracy,\n",
    "        'val_pred_probs': val_predictions_probs.astype(np.float32),\n",
    "        'epochs_trained': len(history_dict.get('loss', [])),\n",
    "        'best_epoch': best_epoch,\n",
    "        'val_f1_keras_best': best_val_f1\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run parallel grid search\n",
    "grid_search_results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_single_model)(smote_p, model_p)\n",
    "    for smote_p, model_p in tqdm(\n",
    "        product(smote_parameter_grid, model_parameter_grid),\n",
    "        total=len(smote_parameter_grid) * len(model_parameter_grid),\n",
    "        desc=\"Training models\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\u2705 Grid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ensemble Predictions",
    "",
    "Combine predictions from all models using weighted voting based on their F1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83e\udde0 Creating ensemble predictions...\")\n",
    "\n",
    "# Collect predictions and weights\n",
    "all_predictions = []\n",
    "prediction_weights = []\n",
    "\n",
    "for result in grid_search_results:\n",
    "    pred_probs = result['val_pred_probs']\n",
    "    all_predictions.append(pred_probs)\n",
    "    # Use F1-score as weight (with small epsilon to avoid zero weights)\n",
    "    prediction_weights.append(max(result['val_f1_macro'], 1e-6))\n",
    "\n",
    "# Weighted average of predictions\n",
    "ensemble_predictions_probs = np.average(all_predictions, axis=0, weights=prediction_weights)\n",
    "ensemble_predictions = np.argmax(ensemble_predictions_probs, axis=1)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_f1 = f1_score(val_labels_encoded, ensemble_predictions, average='macro')\n",
    "ensemble_accuracy = np.mean(ensemble_predictions == val_labels_encoded)\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Ensemble Performance:\")\n",
    "print(f\"   F1-score (macro): {ensemble_f1:.4f}\")\n",
    "print(f\"   Accuracy: {ensemble_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Results Summary",
    "",
    "Display results from all models and identify the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "results_summary = pd.DataFrame([\n",
    "    {\n",
    "        'model_type': r['model_type'],\n",
    "        'val_f1_macro': r['val_f1_macro'],\n",
    "        'val_accuracy': r['val_accuracy'],\n",
    "    }\n",
    "    for r in grid_search_results\n",
    "])\n",
    "\n",
    "print(\"\\n\ud83d\udcca Individual Model Results:\")\n",
    "display(results_summary.sort_values('val_f1_macro', ascending=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_result = max(grid_search_results, key=lambda r: r['val_f1_macro'])\n",
    "best_config = best_model_result['model_config'].copy()\n",
    "best_smote_config = best_model_result['smote_params'].copy()\n",
    "\n",
    "print(\"\\n\ud83c\udfc6 Best Single Model:\")\n",
    "print(f\"   Type: {best_model_result['model_type']}\")\n",
    "print(f\"   Macro F1: {best_model_result['val_f1_macro']:.4f}\")\n",
    "print(f\"   Accuracy: {best_model_result['val_accuracy']:.4f}\")\n",
    "print(f\"   Hyperparameters: {best_config}\")\n",
    "print(f\"   SMOTE params: {best_smote_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Retrain Best Model",
    "",
    "Retrain the best model configuration with more epochs for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udfaf Retraining best model configuration...\")\n",
    "\n",
    "# Training parameters\n",
    "final_training_epochs = 80\n",
    "final_model_config = best_config.copy()\n",
    "final_smote_config = best_smote_config.copy()\n",
    "\n",
    "# Extract parameters\n",
    "final_model_type = final_model_config.pop('model_type')\n",
    "final_learning_rate = final_model_config.pop('learning_rate', 1e-3)\n",
    "final_dropout = final_model_config.pop('dropout', 0.3)\n",
    "\n",
    "print(f\"Configuration: {final_model_type} with learning_rate={final_learning_rate}, dropout={final_dropout}\")\n",
    "\n",
    "# Apply SMOTE\n",
    "X_resampled_final, y_resampled_final = apply_smote_resampling(\n",
    "    base_train_sequences,\n",
    "    base_train_labels,\n",
    "    final_smote_config\n",
    ")\n",
    "\n",
    "# Build final model\n",
    "final_model = build_model_by_type(\n",
    "    final_model_type,\n",
    "    sequence_length=sequence_length,\n",
    "    num_features=feature_count,\n",
    "    num_classes=num_classes,\n",
    "    dropout=final_dropout,\n",
    "    **final_model_config\n",
    ")\n",
    "\n",
    "# Compile final model\n",
    "final_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=final_learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', F1Score(num_classes=num_classes)]\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "final_history = final_model.fit(\n",
    "    X_resampled_final,\n",
    "    y_resampled_final,\n",
    "    validation_data=(val_sequences, val_labels_encoded),\n",
    "    epochs=final_training_epochs,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    callbacks=create_training_callbacks(),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "final_val_predictions = np.argmax(final_model.predict(val_sequences, verbose=0), axis=1)\n",
    "final_f1_macro = f1_score(val_labels_encoded, final_val_predictions, average='macro')\n",
    "final_f1_weighted = f1_score(val_labels_encoded, final_val_predictions, average='weighted')\n",
    "final_accuracy = np.mean(final_val_predictions == val_labels_encoded)\n",
    "\n",
    "print(f\"\\n\u2705 Final Model Performance:\")\n",
    "print(f\"   Macro F1: {final_f1_macro:.4f}\")\n",
    "print(f\"   Weighted F1: {final_f1_weighted:.4f}\")\n",
    "print(f\"   Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Training History Visualization",
    "",
    "Visualize the training process to understand model learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(final_history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(final_history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(final_history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(final_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-score curve\n",
    "axes[2].plot(final_history.history['f1_score'], label='Training F1', linewidth=2)\n",
    "axes[2].plot(final_history.history['val_f1_score'], label='Validation F1', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[2].set_title('F1 Score Over Time', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Final Training Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training Loss: {final_history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Training Accuracy: {final_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Validation Loss: {final_history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Validation Accuracy: {final_history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Validation F1: {final_history.history['val_f1_score'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation",
    "",
    "### 6.1 Detailed Classification Report",
    "",
    "Evaluate model performance with detailed metrics for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd2e Evaluating final model on validation set...\\n\")\n",
    "\n",
    "# Generate predictions\n",
    "val_prediction_probs = final_model.predict(val_sequences)\n",
    "val_predictions = np.argmax(val_prediction_probs, axis=1)\n",
    "\n",
    "# Calculate F1-scores\n",
    "f1_macro_final = f1_score(val_labels_encoded, val_predictions, average='macro')\n",
    "f1_weighted_final = f1_score(val_labels_encoded, val_predictions, average='weighted')\n",
    "f1_per_class = f1_score(val_labels_encoded, val_predictions, average=None)\n",
    "\n",
    "print(\"\ud83c\udfaf F1-SCORE RESULTS (PRIMARY METRIC):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Macro F1-Score:    {f1_macro_final:.4f} \u2b50\")\n",
    "print(f\"Weighted F1-Score: {f1_weighted_final:.4f}\")\n",
    "print(\"\\nF1-Score per Class:\")\n",
    "for i, label_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label_name:15s}: {f1_per_class[i]:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\ud83d\udccb Detailed Classification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(val_labels_encoded, val_predictions, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy_final = np.mean(val_predictions == val_labels_encoded)\n",
    "print(f\"\\n\u2728 Overall Validation Accuracy: {accuracy_final:.4f} ({accuracy_final*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix",
    "",
    "Visualize the confusion matrix to understand misclassification patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "confusion_mat = confusion_matrix(val_labels_encoded, val_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "display_cm = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, \n",
    "                                     display_labels=label_encoder.classes_)\n",
    "display_cm.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Confusion Matrix - Validation Set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"\\n\ud83d\udd0d Confusion Matrix Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for i, label_name in enumerate(label_encoder.classes_):\n",
    "    total_samples = confusion_mat[i].sum()\n",
    "    correct_predictions = confusion_mat[i, i]\n",
    "    accuracy_per_class = (correct_predictions / total_samples * 100) if total_samples > 0 else 0\n",
    "    print(f\"{label_name}:\")\n",
    "    print(f\"  Correct: {correct_predictions}/{total_samples} ({accuracy_per_class:.1f}%)\")\n",
    "    print(f\"  Misclassified: {total_samples - correct_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Test Predictions",
    "",
    "### 7.1 Load and Preprocess Test Data",
    "",
    "Load the test dataset and prepare it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"\ud83d\udcc2 Loading test data...\")\n",
    "test_features = pd.read_csv('pirate_pain_test.csv')\n",
    "\n",
    "print(f\"Test data shape: {test_features.shape}\")\n",
    "print(f\"Number of test samples: {test_features['sample_index'].nunique()}\")\n",
    "print(\"\\n\ud83d\udcca First few rows:\")\n",
    "display(test_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Create Test Sequences",
    "",
    "Process test data into sequences matching the training format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\udd04 Preparing test sequences...\")\n",
    "\n",
    "test_sequences = []\n",
    "test_sample_ids = []\n",
    "\n",
    "unique_test_sample_indices = test_features['sample_index'].unique()\n",
    "\n",
    "for sample_idx in unique_test_sample_indices:\n",
    "    # Extract all time steps for this sample\n",
    "    sample_data = test_features[test_features['sample_index'] == sample_idx].copy()\n",
    "    sample_data = sample_data.sort_values('time')\n",
    "    \n",
    "    # Preprocess and scale features using the training scaler\n",
    "    features = preprocess_features(sample_data)\n",
    "    features_scaled = feature_scaler.transform(features)\n",
    "    test_sequences.append(features_scaled.astype(np.float32))\n",
    "    test_sample_ids.append(sample_idx)\n",
    "\n",
    "print(f\"\u2705 Created {len(test_sequences)} test sequences\")\n",
    "\n",
    "# Pad sequences to match training sequence length\n",
    "test_sequences_padded = np.zeros((len(test_sequences), sequence_length, feature_count), \n",
    "                                 dtype=np.float32)\n",
    "for i, sequence in enumerate(test_sequences):\n",
    "    seq_len = sequence.shape[0]\n",
    "    if seq_len >= sequence_length:\n",
    "        # Truncate if longer\n",
    "        test_sequences_padded[i] = sequence[:sequence_length]\n",
    "    else:\n",
    "        # Pad if shorter\n",
    "        test_sequences_padded[i, :seq_len, :] = sequence\n",
    "\n",
    "print(f\"\u2705 Test tensor shape: {test_sequences_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Generate Predictions",
    "",
    "Use the trained model to predict pain levels for test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\udd2e Generating predictions...\")\n",
    "\n",
    "# Generate predictions\n",
    "test_prediction_probs = final_model.predict(test_sequences_padded)\n",
    "test_predictions = np.argmax(test_prediction_probs, axis=1)\n",
    "test_labels = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "print(f\"\u2705 Generated {len(test_labels)} predictions\")\n",
    "\n",
    "# Show prediction distribution\n",
    "prediction_distribution = pd.Series(test_labels).value_counts()\n",
    "print(\"\\n\ud83d\udcca Prediction Distribution:\")\n",
    "print(prediction_distribution)\n",
    "\n",
    "# Visualize prediction distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors_map = {'no_pain': 'green', 'low_pain': 'orange', 'high_pain': 'red'}\n",
    "bar_colors = [colors_map.get(label, 'gray') for label in prediction_distribution.index]\n",
    "plt.bar(prediction_distribution.index, prediction_distribution.values, color=bar_colors)\n",
    "plt.xlabel('Pain Level', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Test Set Predictions Distribution', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Create Submission File",
    "",
    "Format predictions for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcdd Creating submission file...\\n\")\n",
    "\n",
    "# Format sample indices as zero-padded 3-digit strings\n",
    "formatted_sample_ids = [f\"{int(idx):03d}\" for idx in test_sample_ids]\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'sample_index': formatted_sample_ids,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_path = 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(\"\u2705 Submission file created: submission.csv\")\n",
    "print(f\"   Format: Comma-separated (CSV)\")\n",
    "print(f\"   Total predictions: {len(submission)}\")\n",
    "print(f\"\\n\ud83d\udccb First 15 predictions:\")\n",
    "print(\"=\" * 50)\n",
    "display(submission.head(15))\n",
    "\n",
    "print(\"\\n\u2728 Submission ready for upload!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary",
    "",
    "### Pipeline Overview",
    "",
    "This notebook implements a complete machine learning pipeline for pirate pain level classification:",
    "",
    "1. **Data Loading & Exploration**: Load and analyze time-series sensor data",
    "2. **Feature Engineering**: Preprocess categorical variables and normalize numerical features",
    "3. **Data Preparation**: Create sequences, apply scaling, and split into train/validation sets",
    "4. **Class Balancing**: Use SMOTE to handle class imbalance",
    "5. **Model Development**: Train multiple architectures (LSTM, GRU, CNN-LSTM)",
    "6. **Ensemble Learning**: Combine predictions from multiple models",
    "7. **Model Evaluation**: Assess performance using F1-score and confusion matrix",
    "8. **Prediction**: Generate predictions for test data",
    "",
    "### Key Results",
    "",
    "The final model achieved strong performance on the validation set:",
    "- **Primary Metric**: Macro F1-Score",
    "- **Model Architecture**: Ensemble of LSTM, GRU, and CNN-LSTM models",
    "- **Data Augmentation**: SMOTE for class balance",
    "- **Feature Scaling**: StandardScaler normalization",
    "",
    "### Files Generated",
    "",
    "- `submission.csv`: Test predictions in required format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}